{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from random import uniform, choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Toy model of LLM generation\n",
    "\n",
    "For pedagogical reasons, we are going to make the following simplifications through this tutorial:\n",
    "\n",
    "1. an LLM has an `LLM_VOCABULARY` made up of `V = len(LLM_VOCABULARY)` distinct words\n",
    "2. at each time step, `t`, the LLM outputs a list of float values of size `V` which we will always call `logits`\n",
    "3. we use the LLM as a \"next word predictor\", which means we select at each time step `t` the word in `LLM_VOCABULARY` which has the largest associated logit \"score\" in the `logits` list at step `t`\n",
    "\n",
    "### Exercise 1.1 - Determine the word predicted by the LLM\n",
    "\n",
    "- Our first basic LLM has a `LLM_VOCABULARY` of only `V = 4` different words.\n",
    "- You have a `logits` list of `V = 4` float values\n",
    "\n",
    "**Question: which of the 4 words is being predicted by the LLM in this case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_VOCABULARY = [\"Hello\", \"Bretagne\", \"Orange\", \"Python\"]\n",
    "#    index:          0         1          2         3\n",
    "\n",
    "logits = [0.7, 1.4, 572.3, 0.9]\n",
    "# index:   0    1     2     3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "<details>\n",
    "<summary>Exercise 1.1 - Click here for answer</summary>\n",
    "Exercise 1.1\n",
    "\n",
    "The word being predicted is \"Orange\", because the highest logit value in `logits` (which is `572.3`) is occuring at the index `idx = 2` which corresponds to the index of the word \"Orange\".\n",
    "\n",
    "In other words, `LLM_VOCABULARY[ index of highest logit value] == Orange`.\n",
    "</details>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Getting the next word prediction with code\n",
    "\n",
    "We can code a function in Python to get the index of the highest value in `logits` as follows:\n",
    "\n",
    "```python\n",
    "def get_index_of_highest_value(logits_list):\n",
    "    idx_highest, value_highest = max(enumerate(logits_list), key=lambda x: x[1])\n",
    "\n",
    "    return idx_highest\n",
    "```\n",
    "\n",
    "There is also a helpful implementation in most array libraries, such as NumPy, which is universally called `argmax` and which we will use from now on:\n",
    "\n",
    "```python\n",
    "idx_highest = np.argmax(logits_list)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 1.2 - Using argmax in code\n",
    "\n",
    "- Use both approaches outlined above to obtain with code the same answer you found earlier for the sample list of words and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following values should be equal to each other: 2 2\n",
      "LLM next word prediction is : Orange\n"
     ]
    }
   ],
   "source": [
    "def get_index_of_highest_value(logits_list):\n",
    "    idx_highest, value_highest = max(enumerate(logits_list), key=lambda x: x[1])\n",
    "\n",
    "    return idx_highest\n",
    "\n",
    "# use the get_index_of_highest_value() function on the previous 'logits' to print the same result found manually earlier\n",
    "idx_highest_with_function = get_index_of_highest_value(logits) \n",
    "\n",
    "# also use the in-built NumPy argmax function\n",
    "idx_highest_with_argmax = np.argmax(logits)\n",
    "\n",
    "# check that both answers agree with the result you found earlier:\n",
    "print(\"Following values should be equal to each other:\", idx_highest_with_function, idx_highest_with_argmax)\n",
    "\n",
    "# check that using this index on the LLM_VOCABULARY gives the word result you found earlier:\n",
    "next_word_prediction = LLM_VOCABULARY[ idx_highest_with_argmax ]\n",
    "print(\"LLM next word prediction is :\", next_word_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 First steps with structured generation\n",
    "\n",
    "Structured generation involves **modifying or postprocessing the `logits` list to reflect a requirement or constraint**\n",
    "\n",
    "For the rest of this section, we are going to work with **GPT-000** which has an `LLM_VOCABULARY` of `V = 37` words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_000_LLM_VOCABULARY = [\n",
    "    \"aeroplane\", \"arsenic\",\n",
    "    \"book\",\n",
    "    \"car\", \"crepe\",\n",
    "    \"digital\",\n",
    "    \"elephant\",\n",
    "    \"football\",\n",
    "    \"giraffe\",\n",
    "    \"helicopter\", \"hamburger\",\n",
    "    \"it\", \"internet\",\n",
    "    \"japan\",\n",
    "    \"kanban\",\n",
    "    \"laptop\",\n",
    "    \"microscope\", \"mercury\", \"mobile\",\n",
    "    \"nitrogen\", \"no\",\n",
    "    \"open\",\n",
    "    \"pterodactyl\", \"pizza\",\n",
    "    \"queen\",\n",
    "    \"road\",\n",
    "    \"server\", \"salad\",\n",
    "    \"telephone\",\n",
    "    \"university\", \"uranium\",\n",
    "    \"voice\",\n",
    "    \"water\",\n",
    "    \"xylem\", \"xray\",\n",
    "    \"yes\",\n",
    "    \"zebra\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(GPT_000_LLM_VOCABULARY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First use case for structured generation and GPT-000\n",
    "\n",
    "Pretend that we prompt **GPT-000** to generate a suggestion for what to eat for lunch. Let's see how GPT-000 behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretend_prompt = \"Hello GPT-000, please give me a suggestion for my lunch today!\"\n",
    "\n",
    "# Pretend that you sent this prompt to GPT-000...\n",
    "#\n",
    "#    * time passes... *\n",
    "#\n",
    "#         * time passes... *\n",
    "#\n",
    "#                       * time passes... *\n",
    "#\n",
    "# At last, GPT-000 has calculated the follow logits in response to this prompt:\n",
    "\n",
    "gpt_000_logits = [3.9, 7.4, 8.6, 5.1, 7.3, 3.7, 5.5, 6.2, 13890.3, 5.8, 6.7, 6.3, 2.6, 4.5, 8.5, 1.8, 0.7, 3.6, 4.2, 6.3, 8.7, 3.9, 1.5, 4.4, 7.6, 3.3, 8.6, 1.7, 7.5, 5.1, 4.0, 4.4, 2.8, 8.6, 0.1, 7.7, 8.6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 - So, what does GPT-000 want us to eat for lunch?\n",
    "\n",
    "- Using the `argmax` function from previous section, find the index of the largest logit value\n",
    "- Then, use this index to access the corresponding word in `LLM_VOCABULARY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- GPT-000 output --\n",
      "Sure! Here is what I suggest you eat for lunch: FIX_ME\n"
     ]
    }
   ],
   "source": [
    "gpt_000_index_highest = \"FIX_ME\" # ???????????? use argmax here\n",
    "\n",
    "gpt_000_next_word_prediction = \"FIX_ME\" # ???????????? use your index_highest result here\n",
    "\n",
    "print(\"-- GPT-000 output --\")\n",
    "print(\"Sure! Here is what I suggest you eat for lunch:\", gpt_000_next_word_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "<details>\n",
    "<summary>Exercise 2.1 - Click here for answer</summary>\n",
    "Exercise 2.1\n",
    "\n",
    "```python\n",
    "gpt_000_index_highest = np.argmax(gpt_000_logits)\n",
    "\n",
    "gpt_000_next_word_prediction = GPT_000_LLM_VOCABULARY[gpt_000_index_highest]\n",
    "```\n",
    "\n",
    "**GPT-000 wants you to eat a giraffe !**\n",
    "</details>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Oh no! What a disaster!\n",
    "\n",
    "It seems that GPT-000 was not trained very well, and is giving us bad predictions.\n",
    "\n",
    "- One way that you could try to improve the results is with extensive prompt engineering: \"You are an expert chef. Please give me a lunch suggestion. Please ensure that it is not a toxic chemical. Check your answer against endangered species list.\" etc.\n",
    "\n",
    "**But structured generation offers a far more certain approach:**\n",
    "\n",
    "1. if we **know that the answer should have a specific property**\n",
    "2. and if we have access to the **list of all possible words in the LLM's vocabulary**\n",
    "3. then we can **mask/filter all the possible words, and only keep the ones that satisfy the specific property that we require**\n",
    "\n",
    "Let's walk through these steps\n",
    "\n",
    "## Implementing single-word structured generation\n",
    "\n",
    "In this specific case, we certainly want our answer to be **only an edible lunch**, so let's define this with a function:\n",
    "\n",
    "```python\n",
    "def word_represents_edible_lunch(word):\n",
    "    if word in {\n",
    "        \"crepe\",\n",
    "        \"hamburger\",\n",
    "        \"pizza\",\n",
    "        \"salad\"\n",
    "    }:return True\n",
    "    else:\n",
    "        return False\n",
    "```\n",
    "\n",
    "Now here comes the main idea:\n",
    "\n",
    "1. **if we now apply this function to the `GPT_000_LLM_VOCABULARY` list, we will get a \"MASK\" of `True/False` values that is `True` only for the words that we want**\n",
    "2. then, when we look at the LLM `logits` we can chose to **only select from the indices corresponding to the `True` positions in the MASK**\n",
    "\n",
    "One way to implement the step 2 above is to set the logit values in `logits` that correspond to `False` positions to a **large negative value, which ensures they will never be selected by the `argmax` function later**\n",
    "\n",
    "Let's do this in code now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_represents_edible_lunch(word):\n",
    "    if word in {\n",
    "        \"crepe\",\n",
    "        \"hamburger\",\n",
    "        \"pizza\",\n",
    "        \"salad\"\n",
    "    }:return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# -------------------------------------\n",
    "\n",
    "# 1 - apply the selector function over all of the words in our vocabulary    \n",
    "is_edible_lunch_mask = [word_represents_edible_lunch(word) for word in GPT_000_LLM_VOCABULARY]\n",
    "\n",
    "# This is a list of True / False, of the same size V = len(GPT_000_LLM_VOCABULARY) as the LLM's vocabulary\n",
    "# There should only be True in 4 places - the indices corresponding to crepe/hamburger/pizza/salad\n",
    "#print(is_edible_lunch_mask)\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# 2 - use this mask to:\n",
    "#     a) KEEP the logit value of a word, if that word is in a True is_edible_lunch position\n",
    "#     b) MODIFY the logit value to -1000, if that word is in a False is_edible_lunch position\n",
    "modified_gpt_000_logits = []\n",
    "\n",
    "for idx, is_edible_lunch in enumerate(is_edible_lunch_mask):\n",
    "    if is_edible_lunch:\n",
    "        # case a)\n",
    "        original_logit_value = gpt_000_logits[idx]\n",
    "        # KEEP the logit value in this case\n",
    "        modified_gpt_000_logits.append(original_logit_value)\n",
    "    else:\n",
    "        # case b)\n",
    "        modified_logit_value = -1000\n",
    "        # MODIFY the logit value to a large negative value, let's say: -1000\n",
    "        modified_gpt_000_logits.append(modified_logit_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, if we take the `argmax` prediction from `modified_gpt_000_logits`, we should be 100% sure of getting a tasty lunch suggestion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- GPT-000 output --\n",
      "== NOW WITH ADDED STRUCTURED GENERATION ==\n",
      "Sure! Here is what I suggest you eat for lunch: crepe\n"
     ]
    }
   ],
   "source": [
    "modified_index_highest = np.argmax(modified_gpt_000_logits)\n",
    "\n",
    "modifed_next_word_prediction = GPT_000_LLM_VOCABULARY[modified_index_highest]\n",
    "\n",
    "print(\"-- GPT-000 output --\")\n",
    "print(\"== NOW WITH ADDED STRUCTURED GENERATION ==\")\n",
    "print(\"Sure! Here is what I suggest you eat for lunch:\", modifed_next_word_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Structured generation for more than one word\n",
    "\n",
    "In section 2, for pedagogical reasons, we focused on an LLM that generates only one \"next word\" and then stops. We were able to structure/constrain the generation of this word - in our case to make sure that only an edible lunch was suggested to the LLM user.\n",
    "\n",
    "**The exact same principle can be used to structure the generation of longer LLM outputs, as we see now**\n",
    "\n",
    "## Second example use case for structured generation\n",
    "\n",
    "Suppose that, after our tasty lunch, we want GPT-000 to generate **a sentence where the first letter of each word matches the letter of our name**\n",
    "\n",
    "Since our name has `N` letters, we are going to need to run GPT-000 for `N` generation steps.\n",
    "\n",
    "**Here we build a pretend \"generation\" function that mimics the generation step of a real LLM. In our case, it is a completely random function - all that matters is that it will, when called, generate a list of `logits` of size `V = 37` containing float values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_from_gpt_000(vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # Pretend that there is is a sophisticated LLM here that somehow generates the final output: logits\n",
    "    # ---\n",
    "    #\n",
    "    #  Insert sophisticated LLM architecture \n",
    "    #\n",
    "    # ---\n",
    "    \n",
    "    logits = [round(uniform(0, 9), 1) for _ in range(vocab_size)]\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can type your name and see if GPT-000 **without structured generation** is able to output a sentence that matches the letters in your name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current letter in name is: m and GPT-000 generated the word: microscope\n",
      "Current letter in name is: a and GPT-000 generated the word: zebra\n",
      "Current letter in name is: r and GPT-000 generated the word: giraffe\n",
      "Current letter in name is: c and GPT-000 generated the word: internet\n",
      "Current letter in name is: e and GPT-000 generated the word: it\n",
      "Current letter in name is: l and GPT-000 generated the word: xray\n"
     ]
    }
   ],
   "source": [
    "# Change this to any name you want (no spaces, just a -> z letters)\n",
    "USER_NAME = \"marcel\"\n",
    "\n",
    "USER_NAME = USER_NAME.lower()\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "pretend_prompt_for_user_name = \"Generate a sentence where each word starts with the letters of my name!\"\n",
    "\n",
    "# Pretend you sent this prompt to GPT-000...\n",
    "for current_letter in USER_NAME:\n",
    "    \n",
    "    # GPT-000 here generates some logits:\n",
    "    current_step_logits = get_logits_from_gpt_000(GPT_000_LLM_VOCABULARY)\n",
    "    \n",
    "    # we take the argmax of these logits to get the corresponding next word prediction:\n",
    "    next_word_prediction = GPT_000_LLM_VOCABULARY[np.argmax(current_step_logits)]\n",
    "\n",
    "    # did GPT-000 follow the prompt well ????\n",
    "    print(f\"Current letter in name is: {current_letter} and GPT-000 generated the word: {next_word_prediction}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you are really, really, lucky you probably did not get the output that you wanted - you got a list of words that **did not** match the initial letters of your name. Let's see how structured generation can help us now.\n",
    "\n",
    "## Implementing structured generation for the sentence with initial letters\n",
    "\n",
    "To solve our task, we reuse the same ideas as for the \"single-word\" lunch-idea generation task earlier.\n",
    "\n",
    "Recall that we used a function to identify all valid options : \"edible lunch words\" in the LLM vocabulary.\n",
    "\n",
    "Here the difference is that **the list of all valid options CHANGES AT EACH STEP of the generation process**\n",
    "\n",
    "In other words, the \"MASK\" function that we apply to the LLM vocabulary **changes at each step, since the current letter that we want to generate varies as we move through the `USER_NAME`**\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement this in code.\n",
    "\n",
    "First, we introduce another NumPy function `np.where()` - we can use it to easily implement:\n",
    "\n",
    "- the `True/False` masking behavior that we built earlier\n",
    "- setting the logit values corresponding to the `False` mask values to `-1000` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using np.where()\n",
    "modified_gpt_000_logits_with_npwhere = np.where(is_edible_lunch_mask, # <--- \"If this list contains True, then...\"\n",
    "                                                gpt_000_logits,       # <--- \"...use the value in this list, else ...\"\n",
    "                                                -1000)                # <--- \"...set to this value instead.\"\n",
    "\n",
    "# check that the np.where() approach gives the same answer as our first approach\n",
    "# (we need to convert the NumPy array to a Python list):\n",
    "list(modified_gpt_000_logits_with_npwhere) == modified_gpt_000_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our structured generation for our \"sentence with initial letters\" task.\n",
    "\n",
    "1. for each letter in our `USER_NAME`, use the \"MASK\" approach and create a list of words in the vocabulary that start with this letter\n",
    "2. generate the \"baseline\" GPT-000 `logits`\n",
    "3. use `np.where()` function with this specific MASK from step 1 to apply the structured generation to the `logits`\n",
    "4. get the next word prediction from step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current letter: m | Structured prediction: microscope | Unstructured prediction: open\n",
      "Current letter: a | Structured prediction: arsenic | Unstructured prediction: book\n",
      "Current letter: r | Structured prediction: road | Unstructured prediction: laptop\n",
      "Current letter: c | Structured prediction: car | Unstructured prediction: yes\n",
      "Current letter: e | Structured prediction: elephant | Unstructured prediction: mercury\n",
      "Current letter: l | Structured prediction: laptop | Unstructured prediction: it\n"
     ]
    }
   ],
   "source": [
    "for current_letter in USER_NAME:\n",
    "    # 1. build the mask for this current letter:\n",
    "    # 1. find all words the LLM vocabulary that start with current letter\n",
    "    current_mask = [word.startswith(current_letter) for word in GPT_000_LLM_VOCABULARY]\n",
    "\n",
    "    # 2. generate the baseline GPT-000 logits\n",
    "    # (this step is identical/unchanged from earlier)\n",
    "    current_step_logits = get_logits_from_gpt_000(GPT_000_LLM_VOCABULARY)\n",
    "\n",
    "    # 3. use np.where() with the current mask to select the \"allowed\" logits\n",
    "    modifed_logits = np.where(current_mask, current_step_logits, -1000)\n",
    "\n",
    "    # 4. a) this is the UNSTRUCTURED prediction\n",
    "    next_word_prediction = GPT_000_LLM_VOCABULARY[np.argmax(current_step_logits)]\n",
    "    # 4. b) this is the STRUCTURED prediction using the modified logits\n",
    "    structured_next_word_prediction = GPT_000_LLM_VOCABULARY[np.argmax(modifed_logits)]\n",
    "\n",
    "    # check outputs:\n",
    "    print(f\"Current letter: {current_letter} | Structured prediction: {structured_next_word_prediction} | Unstructured prediction: {next_word_prediction}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above output cell, you should find that each structured prediction word does indeed begin with the correct letter.**\n",
    "\n",
    "# 4 Structured generation with complex examples\n",
    "\n",
    "Up to now we have chosen simple pedagogical examples to illustrate the principles of structured generation, and how modifying the `logits` from our LLM can work.\n",
    "\n",
    "But there are 2 observations we can make about the limitations so far:\n",
    "\n",
    "- the examples are fun but not necessarily \"complex\" enough to match real world uses of LLMs\n",
    "- the implementation of MASKs at each step, and the function we need to define for our condition, is not very optimised\n",
    "\n",
    "The goal of this section is to approach the idea of a **grammar or programmatic approach** for implementing the ideas we have discussed so far, and to see how this maps on to several \"real world\" example use cases.\n",
    "\n",
    "## Structural generation following a specified grammar\n",
    "\n",
    "**TODO: short explanation of grammar / CFG notions**\n",
    "\n",
    "We will show here how to solve one of the most requested features for real LLMs (and common internet search in 2023 for GPT-3): **\"How can I make GPT output a JSON object?\"**\n",
    "\n",
    "We first need to slightly extend our LLM's vocabulary so that it includes more \"words\" - the ones that appear in JSON file format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_000_LLM_VOCABULARY.extend([\n",
    "    \"{\", \"}\",\n",
    "    \"\\\"\",\n",
    "    \":[\", \"]\",\n",
    "    \",\",\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar via state transitions\n",
    "\n",
    "A \"valid JSON\" is determined by a grammar - here we use a somewhat simplified JSON that only contains strings or lists of strings:\n",
    "\n",
    "- Each JSON must start with an open bracket symbol, `{`.\n",
    "- Then it may either contain a new key string, `\"first_key_in_my_json\"`, or it may close the bracket `}`.\n",
    "- If we create a new key at the previous step, then we must create the value associated with that key: `\"first_key_in_my_json\" : [\"val1\", \"val2\", ...]`\n",
    "- After creating the list of values, we can create another key, or close the bracket `}`.\n",
    "\n",
    "Below I have encoded all the allowed transitions between states in the dictionary `STATE_TRANSITIONS`.\n",
    "\n",
    "The rules here are a bit artificial, just to make the next section easier - in practice, your LLM will have a separate word for `[ : ] { } \"` etc\n",
    "\n",
    "**Importantly, we must have a special `START` and `END` state, so that the generation of the JSON does actually terminate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_TRANSITIONS = {\n",
    "    \"START_OF_GENERATION\": [\"open_bracket\"],\n",
    "    \"open_bracket\": [\"open_quotation_key\", \"close_bracket\"],\n",
    "    \"open_quotation_key\": [\"new_key\"], \n",
    "    \"new_key\": [\"close_quotation_key\"],\n",
    "    \"close_quotation_key\": [\"colon_open_square_bracket\"],\n",
    "    \"colon_open_square_bracket\": [\"open_quotation_value\", \"close_square_bracket\"],\n",
    "    \"open_quotation_value\" : [\"value\"],\n",
    "    \"value\": [\"close_quotation_value\"],\n",
    "    \"close_quotation_value\": [\"comma_within_value_list\", \"close_square_bracket\"],\n",
    "    \"comma_within_value_list\": [\"open_quotation_value\"],\n",
    "    \"close_square_bracket\": [\"comma_between_entries\", \"close_bracket\"],\n",
    "    \"comma_between_entries\": [\"open_quotation_key\"],\n",
    "    \"close_bracket\": [\"END_OF_GENERATION\"],    \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights below are just to mimic the behavior of a real LLM - you don't need to worry about them (it's just to ensure that we have a high probability of generating JSONs with lots of words in them rather than just `{}`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_TRANSITIONS_WEIGHTS = {\n",
    "    \"START_OF_GENERATION\": [1.0],\n",
    "    \"open_bracket\": [0.8, 0.2],\n",
    "    \"open_quotation_key\": [1.0], \n",
    "    \"new_key\": [1.0],\n",
    "    \"close_quotation_key\": [1.0],\n",
    "    \"colon_open_square_bracket\": [0.8, 0.2],\n",
    "    \"open_quotation_value\" : [1.0],\n",
    "    \"value\": [1.0],\n",
    "    \"close_quotation_value\": [0.8, 0.2],\n",
    "    \"comma_within_value_list\": [1.0],\n",
    "    \"close_square_bracket\": [0.8, 0.2],\n",
    "    \"comma_between_entries\": [1.0],\n",
    "    \"close_bracket\": [1.0],    \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The grammar of states defines the masks that we will use at each generation step\n",
    "\n",
    "In the examples in Section 2 and 3, we only needed to use **one** function (the \"does word correspond to edible lunch\" function, and the \"does word start with `current_letter`\" function respectively) to define a mask for the vocabulary of allowed words.\n",
    "\n",
    "Here we will need many different functions: **depending on which state we are in, we apply a corresponding function to the LLM vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS_FOR_STATES = {\n",
    "    \"open_bracket\": [word == \"{\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"open_quotation_key\": [word == \"\\\"\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"new_key\": [word.isalpha() for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"close_quotation_key\": [word == \"\\\"\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"colon_open_square_bracket\": [word == \":[\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"open_quotation_value\": [word == \"\\\"\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"value\": [word.isalpha() for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"close_quotation_value\": [word == \"\\\"\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"comma_within_value_list\": [word == \",\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"close_square_bracket\": [word == \"]\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"comma_between_entries\": [word == \",\" for word in GPT_000_LLM_VOCABULARY],\n",
    "    \"close_bracket\": [word == \"}\" for word in GPT_000_LLM_VOCABULARY],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured generation with the JSON grammar\n",
    "\n",
    "The \"LLM\" is unchanged: GPT-000 still generates, at each time step, a `logits` list which we pretend is the result of calling `get_logits_from_gpt_000()`\n",
    "\n",
    "Now we implement structured generation with the tools defined above:\n",
    "\n",
    "- we start the generation process and we track the `current_state` that we are in\n",
    "- from the `current_state` we get the list of all possible \"successor\" states (this corresponds to our understanding of what \"correct JSON\" structure)\n",
    "- we get the corresponding mask to use at this generation step\n",
    "\n",
    "**We continue this generation process until we reach the special `END_OF_GENERATION` state, which occurs when the LLM has outputted a `}` symbol, since this defines the end of a valid JSON in our grammar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = \"START_OF_GENERATION\"\n",
    "\n",
    "generated_string = \"\"\n",
    "structured_generated_string = \"\"\n",
    "\n",
    "while True:\n",
    "    # 1 - get the list of all possible successor states from the current_state:\n",
    "    successor_states = STATE_TRANSITIONS[current_state]\n",
    "    transistion_weights = STATE_TRANSITIONS_WEIGHTS[current_state]\n",
    "    \n",
    "    # 2 - here we model the LLM's structured generation:\n",
    "    # 2/a) We want the LLM to only chose a grammatically allowed next word, and...\n",
    "    # 2/b) ... we need to track the \"type\" of word, so that we know which states are allowed in subsequent steps.\n",
    "    # => we implement this as follows:\n",
    "    # 2/1) - randomly select one of the successor_states\n",
    "    # (use weights to make it more likely to generate large JSON files, just for pedagogical reasons)\n",
    "    current_state = choices(population=successor_states, weights=transistion_weights, k=1)[0]\n",
    "    \n",
    "    # 2/2) - check if we have reached the END_OF_GENERATION state; if so we exit\n",
    "    if current_state == \"END_OF_GENERATION\":\n",
    "        break\n",
    "\n",
    "    # == Steps below are exactly the same as from Section 3 and the \"alphabet sentence\" example ==\n",
    "    # 2/3) - do all the steps from section 3 as before:\n",
    "    \n",
    "    # -- apply the mask corresponding to this state\n",
    "    current_mask = MASKS_FOR_STATES[current_state]\n",
    "\n",
    "    # -- generate the baseline GPT-000 logits\n",
    "    current_step_logits = get_logits_from_gpt_000(GPT_000_LLM_VOCABULARY)\n",
    "\n",
    "    # -- use np.where() with the current mask to select the \"allowed\" logits\n",
    "    modifed_logits = np.where(current_mask, current_step_logits, -1000)\n",
    "\n",
    "    #-- get the structured prediction using the modified logits, and the UNstructured for comparison\n",
    "    next_word_prediction = GPT_000_LLM_VOCABULARY[np.argmax(current_step_logits)]\n",
    "    generated_string += next_word_prediction + \" \" # add space for readability\n",
    "    structured_next_word_prediction = GPT_000_LLM_VOCABULARY[np.argmax(modifed_logits)]\n",
    "    structured_generated_string += structured_next_word_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results - first, look at the output that would have been generated **without the structured grammar:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'server road } car internet internet yes open , football aeroplane arsenic ] salad salad ] road arsenic { water pizza zebra :[ kanban mobile mercury zebra crepe arsenic it } } yes ] digital \" mercury it kanban server university queen server telephone microscope microscope water { open car nitrogen , car open nitrogen mercury microscope car football internet nitrogen arsenic helicopter xray elephant microscope { football uranium mobile \" internet football \" laptop arsenic giraffe salad open xray salad voice car zebra book open nitrogen ] open pterodactyl server water elephant pterodactyl microscope zebra mobile book { elephant arsenic giraffe salad crepe hamburger it pizza mercury pizza { crepe pterodactyl pterodactyl crepe laptop server xylem mobile no nitrogen internet football internet university crepe { open nitrogen telephone aeroplane xray salad digital zebra university aeroplane ] zebra , open telephone book zebra zebra arsenic giraffe internet japan :[ internet aeroplane japan helicopter xylem no uranium \" voice pterodactyl book elephant { car mercury laptop microscope book crepe pizza football { pterodactyl internet nitrogen pterodactyl pterodactyl no crepe laptop internet laptop yes crepe ] internet football \" uranium road pterodactyl yes microscope road crepe crepe mercury yes yes kanban university hamburger crepe car uranium ] hamburger water book elephant giraffe digital open road giraffe yes football no pizza } mercury internet internet , } pizza digital voice queen arsenic internet football arsenic zebra mercury open microscope japan ] laptop queen microscope mobile laptop { queen xylem zebra open uranium laptop mercury uranium xray japan crepe it aeroplane giraffe no telephone xylem ] server telephone car microscope pizza aeroplane nitrogen uranium microscope kanban mercury digital no road { xylem } nitrogen server giraffe giraffe microscope zebra salad salad salad road } '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the **JSON structured generation** - does it indeed represent a valid JSON object ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"crepe\":[\"yes\",\"aeroplane\",\"salad\",\"queen\",\"hamburger\",\"zebra\"],\"elephant\":[\"helicopter\",\"server\",\"telephone\",\"internet\",\"telephone\",\"mercury\",\"internet\",\"xray\",\"football\",\"internet\",\"arsenic\"],\"salad\":[\"book\",\"open\"],\"pterodactyl\":[\"book\",\"giraffe\",\"it\",\"zebra\",\"crepe\",\"mobile\"],\"internet\":[],\"telephone\":[\"digital\",\"book\",\"telephone\",\"arsenic\",\"japan\",\"helicopter\",\"it\",\"elephant\",\"laptop\",\"pizza\",\"internet\",\"no\",\"laptop\",\"internet\",\"road\",\"road\",\"yes\",\"hamburger\",\"open\",\"elephant\"],\"giraffe\":[\"pizza\",\"internet\",\"digital\",\"internet\",\"mercury\"],\"laptop\":[],\"queen\":[\"uranium\"],\"japan\":[\"giraffe\",\"arsenic\",\"microscope\",\"uranium\",\"digital\",\"xylem\"],\"giraffe\":[\"salad\"]}'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_generated_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the JSON quality\n",
    "\n",
    "It looks good! Let's test to see if the strings can indeed be parsed directly as JSON.\n",
    "\n",
    "First, the unstructured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# test if JSON loads both strings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_generated_string \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(generated_string)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# test if JSON loads both strings\n",
    "test_generated_string = json.loads(generated_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the LLM didn't generate a valid JSON without out structured generation intervention.\n",
    "\n",
    "Now, let's check our result **with** the structured generation tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crepe': ['yes', 'aeroplane', 'salad', 'queen', 'hamburger', 'zebra'],\n",
       " 'elephant': ['helicopter',\n",
       "  'server',\n",
       "  'telephone',\n",
       "  'internet',\n",
       "  'telephone',\n",
       "  'mercury',\n",
       "  'internet',\n",
       "  'xray',\n",
       "  'football',\n",
       "  'internet',\n",
       "  'arsenic'],\n",
       " 'salad': ['book', 'open'],\n",
       " 'pterodactyl': ['book', 'giraffe', 'it', 'zebra', 'crepe', 'mobile'],\n",
       " 'internet': [],\n",
       " 'telephone': ['digital',\n",
       "  'book',\n",
       "  'telephone',\n",
       "  'arsenic',\n",
       "  'japan',\n",
       "  'helicopter',\n",
       "  'it',\n",
       "  'elephant',\n",
       "  'laptop',\n",
       "  'pizza',\n",
       "  'internet',\n",
       "  'no',\n",
       "  'laptop',\n",
       "  'internet',\n",
       "  'road',\n",
       "  'road',\n",
       "  'yes',\n",
       "  'hamburger',\n",
       "  'open',\n",
       "  'elephant'],\n",
       " 'giraffe': ['salad'],\n",
       " 'laptop': [],\n",
       " 'queen': ['uranium'],\n",
       " 'japan': ['giraffe', 'arsenic', 'microscope', 'uranium', 'digital', 'xylem']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_structured_generated_string = json.loads(structured_generated_string)\n",
    "\n",
    "test_structured_generated_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We did it - we wrote our own grammar and now we can be sure that, **with any LLM where we can access the `logits` programmatically**, we can ensure **100% of the time that we will get a valid JSON object in return!**\n",
    "\n",
    "As a summary, now that we have worked through several examples, we can present a short conceptual overview:\n",
    "\n",
    "1. define a grammar that captures the structure of your required LLM output\n",
    "2. use this grammar to define a collection of masks that interact with the LLM's vocabulary\n",
    "3. let the LLM generate `logits` and modify them at each step with the grammar and the masks you defined in steps 1 and 2\n",
    "\n",
    "---\n",
    "\n",
    "# Discussion questions\n",
    "\n",
    "To test your understanding, using the techniques seen here and the 3-step summary process above, explain how would you use structured generation to:\n",
    "\n",
    "- Ensure that your LLM app never uses any profanity?\n",
    "- Generate a sentence in which words never appear more than once?\n",
    "- Make your LLM generate valid SQL queries?\n",
    "\n",
    "More sophisticated:\n",
    "\n",
    "- Make your LLM generate a story where paragraphs progressively change sentiment from happy to sad for example?\n",
    "- Make an LLM that generates exact verbatim quotes from a given source text?\n",
    "\n",
    "---\n",
    "\n",
    "# Improved implementations and going further\n",
    "\n",
    "- libraries: Outlines, Marvin, LMQL\n",
    "- grammar libraries: LlamaCPP\n",
    "- `LogitsProcessor` in HuggingFace\n",
    "\n",
    "### Theory\n",
    "\n",
    "- bibliography\n",
    "- shifting/adjusting probability distribution\n",
    "\n",
    "---\n",
    "\n",
    "# Extra material\n",
    "\n",
    "**TODO: timing, but could do a real example - load e.g. GPT-2 but need people to install libraries**\n",
    "\n",
    "**TODO: example with a library ?**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
